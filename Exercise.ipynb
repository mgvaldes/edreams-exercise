{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The objective of this exercise is to be able to accurately predict whether a new customer is interested in buying additional baggage for their trip or not. In general, using past historical data from previous customers we must train a model in order to be use for predictions using future data.\n",
    "\n",
    "We will follow a common and very simple methodology, starting by performing an exploratory analysis in order to get familiarized with the kind of data we'll be dealing with. After we have an idea of the general characteristics of the data we will make decisions about how to handle missing values and transform some features. We later will apply some feature engineering to create new useful features derived from the available data.\n",
    "\n",
    "Before diving deep into the model selection and evaluation process, we create a simple classification model that we'll use as a baseline to improve from that point forward.\n",
    "\n",
    "As for model selection, we'll first try out different types of classification algorithms (Logistic Regresion, Linear Support Vector Machine and Random Forest) using nested cross-validation to evaluate their generalization scores. Based on this results we use the model with highest performance and try to improve it by adding other dimensionality reduction steps before using the classifier such as Principal Component Analysis or Recursive Feature Elimination Feature Selection.\n",
    "\n",
    "Based on the previous results, we perform a simple grid-search with cross-validation using the same options of parameters used in the nested corss-validation process to create a final model to be used with the given test data and generate the required predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "We first load the dataset from the CSV file to take a closer look on to the data. We'll look for missing values, review each variable's type and look at basic descriptions of each variable through statistics or plots depending if the variable is numeric or categorical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the needed libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, LabelBinarizer, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from locale import atof\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv', sep=';', decimal=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See data column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how many missing values the dataset has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are very few missing values, only in variable 'DEVICE'. We'll deal with this issue later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See distribution of classes in target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['EXTRA_BAGGAGE'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTANT: Classes are imbalanced. There are much more samples belonging to the 'False' class than to the 'True' \n",
    "class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review date variables: 'TIMESTAMP', 'DEPARTURE' and 'ARRIVAL'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "date_columns = ['TIMESTAMP', 'DEPARTURE', 'ARRIVAL']\n",
    "print(data[date_columns].head())\n",
    "\n",
    "for i in range(0, len(date_columns)):\n",
    "    print(date_columns[i] + \" type:\", data[date_columns[i]].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These variables don't have the correct type, for now we'll ignore this because we'll later transform them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review categorical variables: 'DEVICE', 'HAUL_TYPE', 'TRIP_TYPE', 'PRODUCT' and 'WEBSITE'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categorical_columns = ['DEVICE', 'HAUL_TYPE', 'TRIP_TYPE', 'PRODUCT', 'WEBSITE']\n",
    "print(data[categorical_columns].head())\n",
    "\n",
    "for i in range(0, len(categorical_columns)):\n",
    "    print(categorical_columns[i] + \" type:\", data[categorical_columns[i]].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These variables don't have the correct type, we'll transform them to categorical variables. \n",
    "\n",
    "IMPORTANT: We are not sure, all levels of each categorical variable are present in the given train dataset. For simplicity we'll assume they all are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['WEBSITE'] = pd.Series(data['WEBSITE'], dtype=\"category\").values\n",
    "\n",
    "data['DEVICE'] = pd.Series(data['DEVICE'], dtype=\"category\").values\n",
    "\n",
    "data['HAUL_TYPE'] = pd.Series(data['HAUL_TYPE'], dtype=\"category\").values\n",
    "\n",
    "data['TRIP_TYPE'] = pd.Series(data['TRIP_TYPE'], dtype=\"category\").values\n",
    "\n",
    "data['PRODUCT'] = pd.Series(data['PRODUCT'], dtype=\"category\").values\n",
    "\n",
    "for i in range(0, len(categorical_columns)):\n",
    "    print(categorical_columns[i] + \" type:\", data[categorical_columns[i]].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets review the frequencies of each level of each categorical variable with respect to the target variable 'EXTRA_BAGGAGE'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ct = pd.crosstab(data.WEBSITE, data.EXTRA_BAGGAGE)\n",
    "\n",
    "ct.plot.barh(stacked=True)\n",
    "plt.legend(title='EXTRA_BAGGAGE')\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 20)\n",
    "plt.rcParams[\"xtick.labelsize\"] = 9\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ct = pd.crosstab(data.DEVICE, data.EXTRA_BAGGAGE)\n",
    "\n",
    "ct.plot.barh(stacked=True)\n",
    "plt.legend(title='EXTRA_BAGGAGE')\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 4)\n",
    "plt.rcParams[\"xtick.labelsize\"] = 9\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ct = pd.crosstab(data.HAUL_TYPE, data.EXTRA_BAGGAGE)\n",
    "\n",
    "ct.plot.barh(stacked=True)\n",
    "plt.legend(title='EXTRA_BAGGAGE')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ct = pd.crosstab(data.TRIP_TYPE, data.EXTRA_BAGGAGE)\n",
    "\n",
    "ct.plot.barh(stacked=True)\n",
    "plt.legend(title='EXTRA_BAGGAGE')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ct = pd.crosstab(data.PRODUCT, data.EXTRA_BAGGAGE)\n",
    "\n",
    "ct.plot.barh(stacked=True)\n",
    "plt.legend(title='EXTRA_BAGGAGE')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous plots it can be seen that the imbalance class problem is present throughout all the levels of each categorical variable. We should keep this in mind and take it into account in future modeling steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review boolean variables: 'TRAIN' and 'SMS'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "boolean_columns = ['TRAIN', 'SMS']\n",
    "print(data[boolean_columns].head())\n",
    "\n",
    "for i in range(0, len(boolean_columns)):\n",
    "    print(boolean_columns[i] + \" type:\", data[boolean_columns[i]].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables have their expected type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review numeric variables: 'GDS', 'NO_GDS', 'ADULTS', 'CHILDREN' and 'INFANTS' as discrete, and 'DISTANCE' as continuous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numeric_columns = ['GDS', 'NO_GDS', 'ADULTS', 'CHILDREN', 'INFANTS', 'DISTANCE']\n",
    "print(data[numeric_columns].head())\n",
    "\n",
    "for i in range(0, len(numeric_columns)):\n",
    "    print(numeric_columns[i] + \" type:\", data[numeric_columns[i]].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables have their expected type. Let's review some statistics related to these numeric variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data[numeric_columns].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything seems to be correct and these numeric variables seem to have reasonable min and max values. The 'DISTANCE' variable has very high values with respect to other variables. This could be an issue in further steps of model selection and training and for this we'll standardize each feature. We'll give more details about this further in the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final exploratory step, we'll plot the correlation between variables in a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_correlation_heatmap(data):\n",
    "    # Compute the correlation matrix\n",
    "    corr = data.corr()\n",
    "\n",
    "    # Generate a mask for the upper triangle\n",
    "    mask = np.zeros_like(corr, dtype=np.bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "    # Set up the matplotlib figure\n",
    "    f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "    # Generate a custom diverging colormap\n",
    "    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "    # Draw the heatmap with the mask and correct aspect ratio\n",
    "    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_correlation_heatmap(data.drop('EXTRA_BAGGAGE', 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows relatively low correlation values between variables. The highest negative correlation is between 'GDS' and 'NO_GDS' variables, which is expected, because as the number of flights bought through the GDS increases, the number of flights bought though other channels decreases.\n",
    "\n",
    "The plot also shows correlations between numeric variables only. Further in the analysis, after transforming some variables, we'll plot again the correlation between variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll clean our dataset in order to make it ready to be used for modeling and predicting. We'll start by dealing with the missing values, followed by perfoming some transformations to the original variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we mentioned above, the only variable that has missing values is the 'DEVICE' variable. Since it has very few missing values with respect to the dataset size we'll just remove them. If there were much more we could use different imputation methods to fill this missing values:\n",
    "\n",
    "- For numeric variables we could perform a simple imputation of the mean, mode or median value of the column. In case the variable is categorical we could impute the most frequent level of the variable.\n",
    "- Another more complex option is to estimate the missing value with the help of regression, ANOVA, logistic regression or another modelling technique.\n",
    "- We could also fill missing values using similarities between samples using algorithms like K-Nearest Neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = data[pd.notnull(data['DEVICE'])]\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see there are no more missing values in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we drop the 'ID' variable since it is useless for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = data.drop('ID', 1)\n",
    "data = data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All date variables have the same format 'day_number/month_name'. We'll transform these values to numeric values corresponding to the number of the week in year that the date belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function that transforms date into value corresponding to number of week in year that it belongs to. We'll assume\n",
    "# dates correspond to this year 2017.\n",
    "def day_to_week_of_year(date_to_transform):\n",
    "    return datetime.strptime(date_to_transform + \"/2017\", '%d/%B/%Y').isocalendar()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['TIMESTAMP'] = data['TIMESTAMP'].apply(day_to_week_of_year)\n",
    "data['DEPARTURE'] = data['DEPARTURE'].apply(day_to_week_of_year)\n",
    "data['ARRIVAL'] = data['ARRIVAL'].apply(day_to_week_of_year)\n",
    "\n",
    "print(data[date_columns].head())\n",
    "\n",
    "for i in range(0, len(date_columns)):\n",
    "    print(date_columns[i] + \" type:\", data[date_columns[i]].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see date variables are transformed to numeric discrete values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the categorical variables, we'll try out two types of transformations:\n",
    "\n",
    "- Using the *LabelEncoder* that transforms each level of the categorical variable to a discrete numeric value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_encoded = data.copy()\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "data_encoded['WEBSITE'] = pd.Series(label_encoder.fit_transform(data_encoded['WEBSITE'])).values\n",
    "\n",
    "data_encoded['DEVICE'] = pd.Series(label_encoder.fit_transform(data_encoded['DEVICE'])).values\n",
    "\n",
    "data_encoded['HAUL_TYPE'] = pd.Series(label_encoder.fit_transform(data_encoded['HAUL_TYPE'])).values\n",
    "\n",
    "data_encoded['TRIP_TYPE'] = pd.Series(label_encoder.fit_transform(data_encoded['TRIP_TYPE'])).values\n",
    "\n",
    "data_encoded['PRODUCT'] = pd.Series(label_encoder.fit_transform(data_encoded['PRODUCT'])).values\n",
    "\n",
    "print(data_encoded[categorical_columns].head())\n",
    "\n",
    "for i in range(0, len(categorical_columns)):\n",
    "    print(categorical_columns[i] + \" type:\", data_encoded[categorical_columns[i]].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before saving the dataset, we'll plot the correlation matrix taking into account categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_correlation_heatmap(data_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the new cleaned and encoded data into a new CSV file called 'clean_encoded_train.csv'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_encoded.to_csv('clean_encoded_train.csv', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using the *LabelBinarizer* that transforms a categorical variable to multiple binary variables, as much as the number of levels of the original categorical variable.\n",
    "\n",
    "For this case, we won't binarize 'WEBSITE' variable because it has too many levels. For this variable we'll use the previous *LabelEncoder*.\n",
    "\n",
    "After transforming the original categorical variables, we'll drop the original ones and only keep the new binarized variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_binarized = data.copy()\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "data_binarized['WEBSITE'] = pd.Series(label_encoder.fit_transform(data_binarized['WEBSITE'])).values\n",
    "\n",
    "label_encoder = LabelBinarizer()\n",
    "\n",
    "encoder_result = label_encoder.fit_transform(data_binarized['DEVICE'])\n",
    "bin_device_columns = [\"DEVICE_\" + str(bin_class) for bin_class in label_encoder.classes_]\n",
    "data_device = pd.DataFrame(encoder_result, columns=bin_device_columns)\n",
    "data_binarized = data_binarized.drop('DEVICE', 1)\n",
    "data_binarized = data_binarized.reset_index(drop=True)\n",
    "data_binarized = pd.concat([data_binarized, data_device], axis=1)\n",
    "data_binarized = data_binarized.reset_index(drop=True)\n",
    "\n",
    "encoder_result = label_encoder.fit_transform(data_binarized['HAUL_TYPE'])\n",
    "bin_haul_type_columns = [\"HAUL_TYPE_\" + str(bin_class) for bin_class in label_encoder.classes_]\n",
    "data_haul_type = pd.DataFrame(encoder_result, columns=bin_haul_type_columns)\n",
    "data_binarized = data_binarized.drop('HAUL_TYPE', 1)\n",
    "data_binarized = data_binarized.reset_index(drop=True)\n",
    "data_binarized = pd.concat([data_binarized, data_haul_type], axis=1)\n",
    "data_binarized = data_binarized.reset_index(drop=True)\n",
    "\n",
    "encoder_result = label_encoder.fit_transform(data_binarized['TRIP_TYPE'])\n",
    "bin_trip_type_columns = [\"TRIP_TYPE_\" + str(bin_class) for bin_class in label_encoder.classes_]\n",
    "data_trip_type = pd.DataFrame(encoder_result, columns=bin_trip_type_columns)\n",
    "data_binarized = data_binarized.drop('TRIP_TYPE', 1)\n",
    "data_binarized = data_binarized.reset_index(drop=True)\n",
    "data_binarized = pd.concat([data_binarized, data_trip_type], axis=1)\n",
    "data_binarized = data_binarized.reset_index(drop=True)\n",
    "\n",
    "encoder_result = label_encoder.fit_transform(data_binarized['PRODUCT'])\n",
    "data_product = pd.Series(encoder_result[:, 0], name='PRODUCT')\n",
    "data_binarized = data_binarized.drop('PRODUCT', 1)\n",
    "data_binarized = data_binarized.reset_index(drop=True)\n",
    "data_binarized = pd.concat([data_binarized, data_product], axis=1)\n",
    "data_binarized = data_binarized.reset_index(drop=True)\n",
    "\n",
    "binarized_categorical_columns = list()\n",
    "binarized_categorical_columns += list(bin_device_columns)\n",
    "binarized_categorical_columns += list(bin_haul_type_columns)\n",
    "binarized_categorical_columns += list(bin_trip_type_columns)\n",
    "binarized_categorical_columns += list(['PRODUCT'])\n",
    "\n",
    "print(data_binarized[binarized_categorical_columns].head())\n",
    "\n",
    "for i in range(0, len(binarized_categorical_columns)):\n",
    "    print(binarized_categorical_columns[i] + \" type:\", data_binarized[binarized_categorical_columns[i]].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the new cleaned and binarized data into a new CSV file called 'clean_binarized_train.csv'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_binarized.to_csv('clean_binarized_train.csv', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll try out a quick test with each clean dataset (encoded and binarized) and a simple classification model such as *LogisticRegression* to see how the model performs. We'll split the data into training and test data, we'll train the model using the training data and keep the model's default parameters. For now we won't perform any grid-search to perform hyper-parameter tunning or any nested cross-validation to obtain a generalization score.\n",
    "\n",
    "We'll use the test data to evaluate the previous model with the F1 score and we'll plot the confussion matrix to visually understand better the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function that evaluates model using F1 score\n",
    "def evaluate_model(best_estimator, X_test, y_test):\n",
    "    y_pred = best_estimator.predict(X_test)    \n",
    "    \n",
    "    f1_eval_score = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    print(\"##### F1 Score:\", f1_eval_score)\n",
    "    print()\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plot_confusion_matrix(cm, ['False', 'True'], title='Confusion matrix', cmap=plt.cm.Purples)\n",
    "    \n",
    "\n",
    "# Function to plot confussion matrix\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Purples):\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    \n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        \n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaned Encoded Data Experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load encoded data\n",
    "data_encoded = pd.read_csv('clean_encoded_train.csv', sep=';', decimal='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split data to separate 'y' and 'X'.\n",
    "de_y = data_encoded['EXTRA_BAGGAGE']\n",
    "\n",
    "# Drop target variable from X DataFrame\n",
    "de_X = data_encoded.drop('EXTRA_BAGGAGE', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into training and test datasets, asumming the 'test.csv' data is future data and the 'train.csv' data is the only one we have available.\n",
    "\n",
    "We'll use the 'stratify' option since previously we saw the target variable was imbalanced and we want to keep the distribution of each class in both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "de_X_train, de_X_test, de_y_train, de_y_test = train_test_split(de_X, de_y, test_size=0.3, random_state=875146, stratify=de_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "de_baseline = LogisticRegression(random_state=621473)\n",
    "\n",
    "de_baseline.fit(de_X_train, de_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluate_model(de_baseline, de_X_test, de_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we said we were going to use only the default parameters of the *LogisticRegression* model, we must consider using the 'class_weight' parameter. If we don't specify an option, the model assumes all classes have the same weight equal to one. Since we know a priori, our samples are imbalanced with respect to the response variable, the 'balanced' option automatically adjusts the weights of each sample inversely proportional to the class frequencies in the input data. Taking this into consideration, we train the model again with this option and see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "de_baseline = LogisticRegression(class_weight='balanced', random_state=621473)\n",
    "\n",
    "de_baseline.fit(de_X_train, de_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluate_model(de_baseline, de_X_test, de_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear from the previous results, both the F1 score and confusion matrix of the first model reflect overfitting. Since we let the model weight each sample equally, and there are much more samples belonging to the 'False' class, we see a higher F1 score and true negatives (TN) in the confusion matrix. On the other hand, the second model shows lower performance score and has higher values of false positives (FP), but less overfitting. Since it takes into account the class imbalance, giving different weigths to samples belonging to each class, we can see a considerable increase in the true positives (TP) of the confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaned Binarized Data Experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load binarized data\n",
    "data_binarized = pd.read_csv('clean_binarized_train.csv', sep=';', decimal='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split data to separate 'y' and 'X'.\n",
    "db_y = data_binarized['EXTRA_BAGGAGE']\n",
    "\n",
    "# Drop target variable from X DataFrame\n",
    "db_X = data_binarized.drop('EXTRA_BAGGAGE', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db_X_train, db_X_test, db_y_train, db_y_test = train_test_split(db_X, db_y, test_size=0.3, random_state=875146, stratify=db_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db_baseline = LogisticRegression(class_weight='balanced', random_state=621473)\n",
    "\n",
    "db_baseline.fit(db_X_train, db_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluate_model(db_baseline, db_X_test, db_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see that we obtain almost the same F1 score for both datasets. Just for practical reasons, we'll use the encoded data for further analysis since it has less variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering is the process of creating new variables from the existing raw features in order to increase the predictive power of the learning algorithms. In fact, during the cleaning process, having transformed the categorical variables to discrete or binary variables is already considered as feature engineering, this process is specifically known as 'factorization' of a categorical feature, but we did it earlier to test our data and create a first baseline model to which we could comapre later. Another common feature engineering technique is called 'binning', which consists of 'cutting' values of a continuous variable into 'bins' so it becomes a categorical variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise we'll create three new variables using pre-existing variables from the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load original data\n",
    "original_data = pd.read_csv('train.csv', sep=';', decimal=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRIP_DAYS Feature\n",
    "\n",
    "This feature will be created using the 'DEPARTURE' and 'ARRIVAL' variables. It will represent the duration in days of the trip. We'll assume 'DEPARTURE' dates corresponde to the current year 2017. If the month of the 'ARRIVAL' date is smaller than the one in the 'DEPARTURE' date, we'll assume this date corresponds to next year 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_trip_days(row):\n",
    "    departure_date = datetime.strptime(row['DEPARTURE'] + \"/2017\", '%d/%B/%Y')\n",
    "    arrival_date = datetime.strptime(row['ARRIVAL'] + \"/2017\", '%d/%B/%Y')\n",
    "    \n",
    "    if arrival_date.month < departure_date.month:\n",
    "        arrival_date = datetime.strptime(row['ARRIVAL'] + \"/2018\", '%d/%B/%Y')\n",
    "    \n",
    "    trip_days = arrival_date - departure_date\n",
    "    \n",
    "    return trip_days.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trip_days = original_data.apply(lambda row: calculate_trip_days(row), axis=1)\n",
    "trip_days = trip_days[pd.notnull(original_data['DEVICE'])]\n",
    "trip_days = trip_days.reset_index(drop=True)\n",
    "\n",
    "data_encoded['TRIP_DAYS'] = trip_days\n",
    "\n",
    "print(data_encoded['TRIP_DAYS'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLANNING_DAYS Feature\n",
    "\n",
    "This feature will be created using the 'TIMESTAMP' and 'DEPARTURE' variables. It will represent how many days ahead of the departure date, the trip was planned, starting from the day the plane tickets were bought."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_planning_days(row):\n",
    "    buy_date = datetime.strptime(row['TIMESTAMP'] + \"/2017\", '%d/%B/%Y')\n",
    "    departure_date = datetime.strptime(row['DEPARTURE'] + \"/2017\", '%d/%B/%Y')\n",
    "    \n",
    "    if departure_date.month < buy_date.month:\n",
    "        departure_date = datetime.strptime(row['DEPARTURE'] + \"/2018\", '%d/%B/%Y')\n",
    "    \n",
    "    planning_days = departure_date - buy_date\n",
    "    \n",
    "    return planning_days.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "planning_days = original_data.apply(lambda row: calculate_planning_days(row), axis=1)\n",
    "planning_days = planning_days[pd.notnull(original_data['DEVICE'])]\n",
    "planning_days = planning_days.reset_index(drop=True)\n",
    "\n",
    "data_encoded['PLANNING_DAYS'] = planning_days\n",
    "\n",
    "print(data_encoded['PLANNING_DAYS'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WEEKEND_DAYS Feature\n",
    "\n",
    "This feature will be created using the 'DEPARTURE' and 'ARRIVAL' variables. It will represent how many weekend days are included in the duration of the trip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_weekend_days(row):\n",
    "    aux = datetime.strptime(row['DEPARTURE'] + \"/2017\", '%d/%B/%Y')\n",
    "    arrival_date = datetime.strptime(row['ARRIVAL'] + \"/2017\", '%d/%B/%Y')\n",
    "    \n",
    "    if arrival_date.month < aux.month:\n",
    "        arrival_date = datetime.strptime(row['ARRIVAL'] + \"/2018\", '%d/%B/%Y')\n",
    "    \n",
    "    weekend_days = 0\n",
    "    \n",
    "    while (aux <= arrival_date):\n",
    "        if (aux.weekday() > 4):\n",
    "            weekend_days += 1\n",
    "            \n",
    "        aux = aux + timedelta(days=1)\n",
    "    \n",
    "    return weekend_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weekend_days = original_data.apply(lambda row: calculate_weekend_days(row), axis=1)\n",
    "weekend_days = weekend_days[pd.notnull(original_data['DEVICE'])]\n",
    "weekend_days = weekend_days.reset_index(drop=True)\n",
    "\n",
    "data_encoded['WEEKEND_DAYS'] = weekend_days\n",
    "\n",
    "print(data_encoded['WEEKEND_DAYS'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We think this kind of variables can give more explicit information to the model in order to perform a better discrimination between passengers that buy or not an extra baggage on their trip. We'll save this new dataset into a separate CSV file called 'clean_enriched_train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_encoded.to_csv('cleaned_enriched_train.csv', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll try this new data with our baseline *LogisticRegression* model to see if our new features improve the model's performance score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load enriched data\n",
    "data_enriched = pd.read_csv('cleaned_enriched_train.csv', sep=';', decimal='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split data to separate 'y' and 'X'.\n",
    "den_y = data_enriched['EXTRA_BAGGAGE']\n",
    "\n",
    "# Drop target variable from X DataFrame\n",
    "den_X = data_enriched.drop('EXTRA_BAGGAGE', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "den_X_train, den_X_test, den_y_train, den_y_test = train_test_split(den_X, den_y, test_size=0.3, random_state=875146, stratify=den_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "den_baseline = LogisticRegression(class_weight='balanced', random_state=621473)\n",
    "\n",
    "den_baseline.fit(den_X_train, den_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluate_model(den_baseline, den_X_test, den_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The enriched data improves the model's performance score by a very small fraction, so from now on we will use these new enriched data for further experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection and Evaluation\n",
    "\n",
    "In this section we'll test various classification algorithms with the same data (enriched) to compare their performance scores. In order to get the most accurate generalization score (most close to reality) of a model, with the least bias, at the same time that we choose the best hyper-parameters, we must perform nested cross-validation. The outer corss-validation will be used to assess the performance of the model. For each of this outer folds we'll perform an inner cross-validation that will be used to determine the hyper-parameters in each fold.\n",
    "\n",
    "More of nested cross-validation can be seen in this paper: Gavin C. Cawley, Nicola L. C. Talbot, \"On Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation\", http://jmlr.csail.mit.edu/papers/v11/cawley10a.html\n",
    "\n",
    "Once we identify the model with the highest generalization score, we re-execute only the inner cross-validation process that performs grid-search along with cross-validation to find the best configuration of hyper-parameters for that model and obtain a unique model trained with the whole training dataset. In practice the cross-validation score obtained for this model will slightly differ from the one obtained in the previous nested cross-validation process. We do the latter in order to be aware of the 'real' generalization score obtained without bias.\n",
    "\n",
    "This model will be the one we'll test with new unseen future data, in this case, the given 'test.csv' data.\n",
    "\n",
    "We'll make use of an important sklearn's class called *Pipeline*, that let's us to sequentially apply a list of transformers to the data and finally apply and estimator model. The purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters. For all models we'll use a *VarianceThreshold* transformer followed by a *StandardScaler* transformer before the classification estimator.\n",
    "\n",
    "The *VarianceThreshold* transformer bassically removes useless features that have zero variance.\n",
    "\n",
    "The *StandardScaler* transformer, as it's name states, standardizes each column to have zero mean and unit variance. This step is important since we are dealing with features with different measurement units, in order to avoid that the variance of one feature (that is orders of magnitude larger than other features) dominate the objective function of certain classification methods, and make the estimator unable to learn from other features correctly as expected, we standardize all the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "We first start by trying out again the *LogisticRegression* classifier but performing hyper-parameter tunning for the penalization parameter 'C'. This parameter represents the inverse of regularization strength. Similar to SVM's the smaller the values, the stronger the regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load cleaned enriched data\n",
    "data_type = 'clean_enriched'\n",
    "data = pd.read_csv(data_type + '_train.csv', sep=';', decimal='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split data to separate 'y' and 'X'.\n",
    "y = data['EXTRA_BAGGAGE']\n",
    "\n",
    "# Drop target variable from X DataFrame\n",
    "X = data.drop('EXTRA_BAGGAGE', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_pipe = Pipeline([('variance', VarianceThreshold()),\n",
    "                    ('scaler', StandardScaler()),\n",
    "                    ('lr', LogisticRegression(class_weight='balanced', random_state=621473))])\n",
    "\n",
    "param_grid = dict()\n",
    "param_grid['lr__C'] = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "\n",
    "inner_cv = StratifiedKFold(n_splits=10, random_state=975428)\n",
    "outer_cv = StratifiedKFold(n_splits=10, random_state=248733)\n",
    "\n",
    "gs_cv = GridSearchCV(lr_pipe, param_grid=param_grid, n_jobs=-1, scoring='f1_weighted', cv=inner_cv, verbose=10)\n",
    "\n",
    "lr_nested_cv_f1_scores = cross_val_score(gs_cv, X, y, cv=outer_cv, verbose=10)\n",
    "\n",
    "joblib.dump(lr_nested_cv_f1_scores, data_type + '_lr_nested_cv_f1_scores.pkl', compress=1)\n",
    "\n",
    "print(\"##### Generalization F1 Score: mean =\", np.mean(lr_nested_cv_f1_scores), \"std =\", np.std(lr_nested_cv_f1_scores))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous execution gives the following F1 generalization score:\n",
    "\n",
    "##### Generalization F1 Score: mean = 0.668949634197 std = 0.00667454322097\n",
    "\n",
    "We'll also plot the outer cross-validation scores in a boxplot to assess the model's statibility. If there are very few or no outlierts, this is a good sign that the model is stable. Since we have a small standard deviation we expect to have no outliers in the boxplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to plot boxplot of nested cross-validation scores.\n",
    "def plot_cv_scores(cv_scores):\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    \n",
    "    ax = sns.boxplot(x=cv_scores, orient=\"v\")\n",
    "    \n",
    "    plt.ylim(0, 1) \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_nested_cv_f1_scores = joblib.load(data_type + '_lr_nested_cv_f1_scores.pkl')\n",
    "\n",
    "plot_cv_scores(lr_nested_cv_f1_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear SVM\n",
    "\n",
    "As a second linear classification model we'll try *LinearSVC* classifier, performing hyper-parameter tunning for the penalization parameter 'C'. This parameter represents penalty given to the error term. The smaller the values, the stronger the regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linear_svm_pipe = Pipeline([('variance', VarianceThreshold()),\n",
    "                            ('scaler', StandardScaler()),\n",
    "                            ('linear_svm', LinearSVC(penalty='l1', dual=False, random_state=123456, class_weight='balanced'))])\n",
    "\n",
    "param_grid = dict()\n",
    "param_grid['linear_svm__C'] = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "\n",
    "inner_cv = StratifiedKFold(n_splits=10, random_state=975428)\n",
    "outer_cv = StratifiedKFold(n_splits=10, random_state=248733)\n",
    "\n",
    "gs_cv = GridSearchCV(linear_svm_pipe, param_grid=param_grid, n_jobs=-1, scoring='f1_weighted', cv=inner_cv, verbose=10)\n",
    "\n",
    "linear_svm_nested_cv_f1_scores = cross_val_score(gs_cv, X, y, cv=outer_cv, verbose=10)\n",
    "\n",
    "joblib.dump(linear_svm_nested_cv_f1_scores, data_type + '_linear_svm_nested_cv_f1_scores.pkl', compress=1)\n",
    "\n",
    "print(\"##### Generalization F1 Score: mean =\", np.mean(linear_svm_nested_cv_f1_scores), \"std =\", np.std(linear_svm_nested_cv_f1_scores))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F1 generalization score is shown below:\n",
    "\n",
    "##### Generalization F1 Score: mean = 0.667330992787 std = 0.00671589397452"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linear_svm_nested_cv_f1_scores = joblib.load(data_type + '_linear_svm_nested_cv_f1_scores.pkl')\n",
    "\n",
    "plot_cv_scores(linear_svm_nested_cv_f1_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting boxplot of the outer cross-validation scores still shows model stability even though the generalization score obtained was a bit lower than the one obtained with *LogisticRegression*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "Finally we test a non-linear ensemble method called the *RandomForestClassifier*. We change the default value for the 'max_features' parameter to use a common rule of thumb, the 'sqrt' option. This parameter represents the number of features to consider when looking for the best split in a tree. By using the the 'sqrt' option we make the model use as 'max_features', the square root of the total number of features in the data.\n",
    "\n",
    "This estimator has several parameters available to tune. For simplicity, we'll perform hyper-parameter tunning only for the 'n_estimators' parameter. This parameter represents the number of trees we want to build before taking the maximum voting or averages of predictions. The higher the number of trees the more stronger and stable the predictions will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_pipe = Pipeline([('variance', VarianceThreshold()),\n",
    "                    ('scaler', StandardScaler()),\n",
    "                    ('rf', RandomForestClassifier(max_features='sqrt', oob_score=True, random_state=573146, class_weight='balanced'))])\n",
    "\n",
    "param_grid = dict()\n",
    "param_grid['rf__n_estimators'] = list(range(200, 2300, 300))\n",
    "\n",
    "inner_cv = StratifiedKFold(n_splits=10, random_state=975428)\n",
    "outer_cv = StratifiedKFold(n_splits=10, random_state=248733)\n",
    "\n",
    "gs_cv = GridSearchCV(rf_pipe, param_grid=param_grid, n_jobs=-1, scoring='f1_weighted', cv=inner_cv, verbose=10)\n",
    "\n",
    "rf_nested_cv_f1_scores = cross_val_score(gs_cv, X, y, cv=outer_cv, verbose=10)\n",
    "\n",
    "joblib.dump(rf_nested_cv_f1_scores, data_type + '_rf_nested_cv_f1_scores.pkl', compress=1)\n",
    "\n",
    "print(\"##### Generalization F1 Score: mean =\", np.mean(rf_nested_cv_f1_scores), \"std =\", np.std(rf_nested_cv_f1_scores))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F1 generalization score is shown below:\n",
    "\n",
    "##### Generalization F1 Score: \n",
    "\n",
    "This classifier is the one with best generalization score. For this we'll add to this particular pipeline, other transformers to see if can improve even more the generalization score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_nested_cv_f1_scores = joblib.load(data_type + '_rf_nested_cv_f1_scores.pkl')\n",
    "\n",
    "plot_cv_scores(rf_nested_cv_f1_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting boxplot of the outer cross-validation scores also show model stability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis + Random Forest\n",
    "\n",
    "*PCA* is a linear dimensionality reduction technique that uses the singular value decomposition of the data to project it to a lower dimensional space. The idea is to find the plane upon which the cloud of points will be projected, that maximizes the projected inertia or variance, this is, that maximazes the original cloud information.\n",
    "\n",
    "We'll add this method as another transformation of the data before applying the estimator to calculate the generalization score of the whole model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca_rf_pipe = Pipeline([('variance', VarianceThreshold()),\n",
    "                        ('scaler', StandardScaler()),\n",
    "                        ('pca', PCA(random_state=554197)),\n",
    "                        ('rf', RandomForestClassifier(max_features='sqrt', oob_score=True, random_state=573146, class_weight='balanced'))])\n",
    "\n",
    "param_grid = dict()\n",
    "param_grid['rf__n_estimators'] = list(range(200, 2300, 300))\n",
    "param_grid['pca__n_components'] = list(range(2, 14, 2))\n",
    "\n",
    "inner_cv = StratifiedKFold(n_splits=10, random_state=975428)\n",
    "outer_cv = StratifiedKFold(n_splits=10, random_state=248733)\n",
    "\n",
    "gs_cv = GridSearchCV(pca_rf_pipe, param_grid=param_grid, n_jobs=-1, scoring='f1_weighted', cv=inner_cv, verbose=10)\n",
    "\n",
    "pca_rf_nested_cv_f1_scores = cross_val_score(gs_cv, X, y, cv=outer_cv, verbose=10)\n",
    "\n",
    "joblib.dump(pca_rf_nested_cv_f1_scores, data_type + '_pca_rf_nested_cv_f1_scores.pkl', compress=1)\n",
    "\n",
    "print(\"##### Generalization F1 Score: mean =\", np.mean(pca_rf_nested_cv_f1_scores), \"std =\", np.std(pca_rf_nested_cv_f1_scores))\n",
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F1 generalization score is shown below:\n",
    "\n",
    "##### Generalization F1 Score: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca_rf_nested_cv_f1_scores = joblib.load(data_type + '_pca_rf_nested_cv_f1_scores.pkl')\n",
    "\n",
    "plot_cv_scores(pca_rf_nested_cv_f1_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recursive Feature Elimination + Random Forest\n",
    "\n",
    "*RFE* is a wrapper feature selection method used to reduce the number of avaliable features to a smaller subset with higher predictive power with respect to a specific model. The latter refers to the predictive power of an estimator that is 'wrapped' and used throughout several iterations to determine the best subset of features to keep. The general goal of RFE recursively consider smaller and smaller sets of features until final number of desired features is reached.\n",
    "\n",
    "In our experiment we'll 'wrapp' a simple *LogisticRegression* estimator around the *RFE* and put it as a transformation step before applying the *RandomForestClassifier*. Just for time performance issues we'll do a small hyper-parameter tunning of the 'C' parameter of the wrapped *LogisticRegression*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rfe_rf_pipe = Pipeline([('variance', VarianceThreshold()),\n",
    "                        ('scaler', StandardScaler()),\n",
    "                        ('rfe', RFE(LogisticRegression(class_weight='balanced', random_state=348744), step=4)),\n",
    "                        ('rf', RandomForestClassifier(max_features='sqrt', oob_score=True, random_state=573146, class_weight='balanced'))])\n",
    "\n",
    "param_grid = dict()\n",
    "param_grid['rf__n_estimators'] = list(range(200, 2300, 300))\n",
    "param_grid['rfe__n_features_to_select'] = list(range(5, 18, 5))\n",
    "param_grid['rfe__estimator__C'] = [0.1, 1, 10]\n",
    "\n",
    "inner_cv = StratifiedKFold(n_splits=10, random_state=975428)\n",
    "outer_cv = StratifiedKFold(n_splits=10, random_state=248733)\n",
    "\n",
    "gs_cv = GridSearchCV(rfe_lr_pipe, param_grid=param_grid, n_jobs=-1, scoring='f1_weighted', cv=inner_cv, verbose=10)\n",
    "\n",
    "rfe_rf_nested_cv_f1_scores = cross_val_score(gs_cv, X, y, cv=outer_cv, verbose=10)\n",
    "\n",
    "joblib.dump(rfe_rf_nested_cv_f1_scores, data_type + '_rfe_rf_nested_cv_f1_scores.pkl', compress=1)\n",
    "\n",
    "print(\"##### Generalization F1 Score: mean =\", np.mean(rfe_rf_nested_cv_f1_scores), \"std =\", np.std(rfe_rf_nested_cv_f1_scores))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F1 generalization score is shown below:\n",
    "\n",
    "##### Generalization F1 Score: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rfe_rf_nested_cv_f1_scores = joblib.load(data_type + '_rfe_rf_nested_cv_f1_scores.pkl')\n",
    "\n",
    "plot_cv_scores(rfe_rf_nested_cv_f1_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Estimator\n",
    "\n",
    "From the previous experiments we've managed to find a classification model (Pipeline) with the highest generalization score: *ALGO*.\n",
    "\n",
    "Now, as we said at the begining of this section, in order to know the best hyper-parameter setting for the model and in order to have a unique model to be used for future predictions we perform a simple grid-search with cross-validation using the same parameter options used in the nested cross-validation process.\n",
    "\n",
    "From the latter process, we'll keep as 'Best Estimator', the one with highest cross-validation F1 score.\n",
    "\n",
    "In order to test our 'Best Estimator' we should make the same transformations we did to the training data, now to the provided test data. Afterwards we predict the target variable 'EXTRA_BAGGAGE' feeding this transformed test data into the 'Best Estimator'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('test.csv', sep=';', decimal=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data = test_data[pd.notnull(test_data['DEVICE'])]\n",
    "test_data = test_data.reset_index(drop=True)\n",
    "\n",
    "print(test_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = test_data.drop('ID', 1)\n",
    "test_data = test_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "date_columns = ['TIMESTAMP', 'DEPARTURE', 'ARRIVAL']\n",
    "\n",
    "test_data['TIMESTAMP'] = test_data['TIMESTAMP'].apply(day_to_week_of_year)\n",
    "test_data['DEPARTURE'] = test_data['DEPARTURE'].apply(day_to_week_of_year)\n",
    "test_data['ARRIVAL'] = test_data['ARRIVAL'].apply(day_to_week_of_year)\n",
    "\n",
    "print(test_data[date_columns].head())\n",
    "\n",
    "for i in range(0, len(date_columns)):\n",
    "    print(date_columns[i] + \" type:\", test_data[date_columns[i]].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "categorical_columns = ['DEVICE', 'HAUL_TYPE', 'TRIP_TYPE', 'PRODUCT', 'WEBSITE']\n",
    "\n",
    "test_data_encoded = test_data.copy()\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "test_data_encoded['WEBSITE'] = pd.Series(label_encoder.fit_transform(test_data_encoded['WEBSITE'])).values\n",
    "\n",
    "test_data_encoded['DEVICE'] = pd.Series(label_encoder.fit_transform(test_data_encoded['DEVICE'])).values\n",
    "\n",
    "test_data_encoded['HAUL_TYPE'] = pd.Series(label_encoder.fit_transform(test_data_encoded['HAUL_TYPE'])).values\n",
    "\n",
    "test_data_encoded['TRIP_TYPE'] = pd.Series(label_encoder.fit_transform(test_data_encoded['TRIP_TYPE'])).values\n",
    "\n",
    "test_data_encoded['PRODUCT'] = pd.Series(label_encoder.fit_transform(test_data_encoded['PRODUCT'])).values\n",
    "\n",
    "print(test_data_encoded[categorical_columns].head())\n",
    "\n",
    "for i in range(0, len(categorical_columns)):\n",
    "    print(categorical_columns[i] + \" type:\", test_data_encoded[categorical_columns[i]].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load original data\n",
    "original_test_data = pd.read_csv('test.csv', sep=';', decimal=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trip_days = original_test_data.apply(lambda row: calculate_trip_days(row), axis=1)\n",
    "trip_days = trip_days[pd.notnull(original_test_data['DEVICE'])]\n",
    "trip_days = trip_days.reset_index(drop=True)\n",
    "\n",
    "test_data_encoded['TRIP_DAYS'] = trip_days\n",
    "\n",
    "print(test_data_encoded['TRIP_DAYS'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "planning_days = original_test_data.apply(lambda row: calculate_planning_days(row), axis=1)\n",
    "planning_days = planning_days[pd.notnull(original_test_data['DEVICE'])]\n",
    "planning_days = planning_days.reset_index(drop=True)\n",
    "\n",
    "test_data_encoded['PLANNING_DAYS'] = planning_days\n",
    "\n",
    "print(test_data_encoded['PLANNING_DAYS'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weekend_days = original_test_data.apply(lambda row: calculate_weekend_days(row), axis=1)\n",
    "weekend_days = weekend_days[pd.notnull(original_test_data['DEVICE'])]\n",
    "weekend_days = weekend_days.reset_index(drop=True)\n",
    "\n",
    "test_data_encoded['WEEKEND_DAYS'] = weekend_days\n",
    "\n",
    "print(test_data_encoded['WEEKEND_DAYS'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data_encoded.to_csv('cleaned_enriched_test.csv', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load cleaned enriched data\n",
    "data_type = 'clean_enriched'\n",
    "test_data = pd.read_csv(data_type + '_test.csv', sep=';', decimal='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs_cv = joblib.load(data_type + '_rf_gs_cv.pkl')\n",
    "\n",
    "best_estimator = gs_cv.best_estimator\n",
    "\n",
    "y_pred = best_estimator.predict(test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
