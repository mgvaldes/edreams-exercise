{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "We first load the dataset from the CSV file to take a closer look on to the data. We'll look for missing values, review each variable's type and look at basic descriptions of each variable through statistics or plots depending if the variable is numeric or categorical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the needed libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, LabelBinarizer, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from locale import atof\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv', sep=';', decimal=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See data column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how many missing values the dataset has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are very few missing values, only in variable 'DEVICE'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See distribution of classes in target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['EXTRA_BAGGAGE'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTANT: Classes are imbalanced. There are much more samples belonging to the 'False' class than to the 'True' \n",
    "class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review date variables: 'TIMESTAMP', 'DEPARTURE' and 'ARRIVAL'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "date_columns = ['TIMESTAMP', 'DEPARTURE', 'ARRIVAL']\n",
    "print(data[date_columns].head())\n",
    "\n",
    "for i in range(0, len(date_columns)):\n",
    "    print(date_columns[i] + \" type:\", data[date_columns[i]].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These variables don't have the correct type, for now we'll ignore this because we'll later transform this variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review categorical variables: 'DEVICE', 'HAUL_TYPE', 'TRIP_TYPE', 'PRODUCT' and 'WEBSITE'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "categorical_columns = ['DEVICE', 'HAUL_TYPE', 'TRIP_TYPE', 'PRODUCT', 'WEBSITE']\n",
    "print(data[categorical_columns].head())\n",
    "\n",
    "for i in range(0, len(categorical_columns)):\n",
    "    print(categorical_columns[i] + \" type:\", data[categorical_columns[i]].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These variables don't have the correct type, we'll transform them to categorical variables. \n",
    "\n",
    "IMPORTANT: We are not sure, all levels of each categorical variable are present in the given train dataset. For simplicity we'll assume they all are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['WEBSITE'] = pd.Series(data['WEBSITE'], dtype=\"category\").values\n",
    "\n",
    "data['DEVICE'] = pd.Series(data['DEVICE'], dtype=\"category\").values\n",
    "\n",
    "data['HAUL_TYPE'] = pd.Series(data['HAUL_TYPE'], dtype=\"category\").values\n",
    "\n",
    "data['TRIP_TYPE'] = pd.Series(data['TRIP_TYPE'], dtype=\"category\").values\n",
    "\n",
    "data['PRODUCT'] = pd.Series(data['PRODUCT'], dtype=\"category\").values\n",
    "\n",
    "for i in range(0, len(categorical_columns)):\n",
    "    print(categorical_columns[i] + \" type:\", data[categorical_columns[i]].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets review the frequencies of each level of each categorical variable with respect to the target variable 'EXTRA_BAGGAGE'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ct = pd.crosstab(data.WEBSITE, data.EXTRA_BAGGAGE)\n",
    "\n",
    "ct.plot.barh(stacked=True)\n",
    "plt.legend(title='EXTRA_BAGGAGE')\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 20)\n",
    "plt.rcParams[\"xtick.labelsize\"] = 9\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ct = pd.crosstab(data.DEVICE, data.EXTRA_BAGGAGE)\n",
    "\n",
    "ct.plot.barh(stacked=True)\n",
    "plt.legend(title='EXTRA_BAGGAGE')\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 4)\n",
    "plt.rcParams[\"xtick.labelsize\"] = 9\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ct = pd.crosstab(data.HAUL_TYPE, data.EXTRA_BAGGAGE)\n",
    "\n",
    "ct.plot.barh(stacked=True)\n",
    "plt.legend(title='EXTRA_BAGGAGE')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ct = pd.crosstab(data.TRIP_TYPE, data.EXTRA_BAGGAGE)\n",
    "\n",
    "ct.plot.barh(stacked=True)\n",
    "plt.legend(title='EXTRA_BAGGAGE')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ct = pd.crosstab(data.PRODUCT, data.EXTRA_BAGGAGE)\n",
    "\n",
    "ct.plot.barh(stacked=True)\n",
    "plt.legend(title='EXTRA_BAGGAGE')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review boolean variables: 'TRAIN' and 'SMS'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boolean_columns = ['TRAIN', 'SMS']\n",
    "print(data[boolean_columns].head())\n",
    "\n",
    "for i in range(0, len(boolean_columns)):\n",
    "    print(boolean_columns[i] + \" type:\", data[boolean_columns[i]].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables have their expected type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review numeric variables: 'GDS', 'NO_GDS', 'ADULTS', 'CHILDREN' and 'INFANTS' as discrete, and 'DISTANCE' as continuous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numeric_columns = ['GDS', 'NO_GDS', 'ADULTS', 'CHILDREN', 'INFANTS', 'DISTANCE']\n",
    "print(data[numeric_columns].head())\n",
    "\n",
    "for i in range(0, len(numeric_columns)):\n",
    "    print(numeric_columns[i] + \" type:\", data[numeric_columns[i]].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables have their expected type. Let's review some statistics related to these numeric variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data[numeric_columns].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything seems to be correct and these numeric variables seem to have reasonable min and max values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final exploratory step, we'll plot the correlation between variables in a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_correlation_heatmap(data):\n",
    "    # Compute the correlation matrix\n",
    "    corr = data.corr()\n",
    "\n",
    "    # Generate a mask for the upper triangle\n",
    "    mask = np.zeros_like(corr, dtype=np.bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "    # Set up the matplotlib figure\n",
    "    f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "    # Generate a custom diverging colormap\n",
    "    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "    # Draw the heatmap with the mask and correct aspect ratio\n",
    "    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_correlation_heatmap(data.drop('EXTRA_BAGGAGE', 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows relatively low correlation values between variables. The highest negative correlation is between 'GDS' and 'NO_GDS' variables, which is expected, because as the number of flights bought through the GDS increases, the number of flights bought though other channels decreases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll clean our dataset in order to be ready to be used for modeling and predicting. We'll start by dealing with the missing values, followed by perfoming some transformations to the original variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we mentioned above, the only variable that has missing values is the 'DEVICE' variable. Since it has very few missing values, with respect to the dataset size we'll just remove them. If there were much more we could use different imputation methods to fill this missing values:\n",
    "\n",
    "- For numeric variables we could perform a simple imputation of the mean, mode or median value of the column. In case the variable is categorical we could impute the most frequent level of the variable.\n",
    "- Another more complex option is to estimate the missing value with the help of regression, ANOVA, logistic regression or another modelling technique.\n",
    "- We could also fill missing values using similarities between samples using algorithms like K-Nearest Neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = data[pd.notnull(data['DEVICE'])]\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see there are no more missing values in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later we drop the 'ID' variable since it is useless for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = data.drop('ID', 1)\n",
    "data = data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All date variables have the same format 'day_number/month_name'. We'll transform these values to numeric values corresponding to number of week in year that the date belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function that transforms date into value corresponding to number of week in year that it belongs to. We'll assume\n",
    "# dates correspond to this year 2017.\n",
    "def day_to_week_of_year(date_to_transform):\n",
    "    return datetime.strptime(date_to_transform + \"/2017\", '%d/%B/%Y').isocalendar()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['TIMESTAMP'] = data['TIMESTAMP'].apply(day_to_week_of_year)\n",
    "data['DEPARTURE'] = data['DEPARTURE'].apply(day_to_week_of_year)\n",
    "data['ARRIVAL'] = data['ARRIVAL'].apply(day_to_week_of_year)\n",
    "\n",
    "print(data[date_columns].head())\n",
    "\n",
    "for i in range(0, len(date_columns)):\n",
    "    print(date_columns[i] + \" type:\", data[date_columns[i]].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see date variables are transformed to numeric discrete values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the categorical variables, we'll try out two types of transformations:\n",
    "\n",
    "- Using the LabelEncoder that transforms each level of the categorical variable to a discrete numeric value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_encoded = data.copy()\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "data_encoded['WEBSITE'] = pd.Series(label_encoder.fit_transform(data_encoded['WEBSITE'])).values\n",
    "\n",
    "data_encoded['DEVICE'] = pd.Series(label_encoder.fit_transform(data_encoded['DEVICE'])).values\n",
    "\n",
    "data_encoded['HAUL_TYPE'] = pd.Series(label_encoder.fit_transform(data_encoded['HAUL_TYPE'])).values\n",
    "\n",
    "data_encoded['TRIP_TYPE'] = pd.Series(label_encoder.fit_transform(data_encoded['TRIP_TYPE'])).values\n",
    "\n",
    "data_encoded['PRODUCT'] = pd.Series(label_encoder.fit_transform(data_encoded['PRODUCT'])).values\n",
    "\n",
    "print(data_encoded[categorical_columns].head())\n",
    "\n",
    "for i in range(0, len(categorical_columns)):\n",
    "    print(categorical_columns[i] + \" type:\", data_encoded[categorical_columns[i]].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the new cleaned and encoded data into a new CSV file called 'clean_encoded_train.csv'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_encoded.to_csv('clean_encoded_train.csv', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using the LabelBinarizer that transforms a categorical variable to multiple binary variables, as much as the number of levels of the original categorical variable.\n",
    "\n",
    "For this case, we won't binarize 'WEBSITE' variable because it has too many levels. For this variable we'll use the previous LabelEncoder.\n",
    "\n",
    "After transforming the original categorical variable, we'll drop the original one and only keep the new binarized variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_binarized = data.copy()\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "data_binarized['WEBSITE'] = pd.Series(label_encoder.fit_transform(data_binarized['WEBSITE'])).values\n",
    "\n",
    "label_encoder = LabelBinarizer()\n",
    "\n",
    "encoder_result = label_encoder.fit_transform(data_binarized['DEVICE'])\n",
    "bin_device_columns = [\"DEVICE_\" + str(bin_class) for bin_class in label_encoder.classes_]\n",
    "data_device = pd.DataFrame(encoder_result, columns=bin_device_columns)\n",
    "data_binarized = data_binarized.drop('DEVICE', 1)\n",
    "data_binarized = data_binarized.reset_index(drop=True)\n",
    "data_binarized = pd.concat([data_binarized, data_device], axis=1)\n",
    "data_binarized = data_binarized.reset_index(drop=True)\n",
    "\n",
    "encoder_result = label_encoder.fit_transform(data_binarized['HAUL_TYPE'])\n",
    "bin_haul_type_columns = [\"HAUL_TYPE_\" + str(bin_class) for bin_class in label_encoder.classes_]\n",
    "data_haul_type = pd.DataFrame(encoder_result, columns=bin_haul_type_columns)\n",
    "data_binarized = data_binarized.drop('HAUL_TYPE', 1)\n",
    "data_binarized = data_binarized.reset_index(drop=True)\n",
    "data_binarized = pd.concat([data_binarized, data_haul_type], axis=1)\n",
    "data_binarized = data_binarized.reset_index(drop=True)\n",
    "\n",
    "encoder_result = label_encoder.fit_transform(data_binarized['TRIP_TYPE'])\n",
    "bin_trip_type_columns = [\"TRIP_TYPE_\" + str(bin_class) for bin_class in label_encoder.classes_]\n",
    "data_trip_type = pd.DataFrame(encoder_result, columns=bin_trip_type_columns)\n",
    "data_binarized = data_binarized.drop('TRIP_TYPE', 1)\n",
    "data_binarized = data_binarized.reset_index(drop=True)\n",
    "data_binarized = pd.concat([data_binarized, data_trip_type], axis=1)\n",
    "data_binarized = data_binarized.reset_index(drop=True)\n",
    "\n",
    "encoder_result = label_encoder.fit_transform(data_binarized['PRODUCT'])\n",
    "data_product = pd.Series(encoder_result[:, 0], name='PRODUCT')\n",
    "data_binarized = data_binarized.drop('PRODUCT', 1)\n",
    "data_binarized = data_binarized.reset_index(drop=True)\n",
    "data_binarized = pd.concat([data_binarized, data_product], axis=1)\n",
    "data_binarized = data_binarized.reset_index(drop=True)\n",
    "\n",
    "binarized_categorical_columns = list()\n",
    "binarized_categorical_columns += list(bin_device_columns)\n",
    "binarized_categorical_columns += list(bin_haul_type_columns)\n",
    "binarized_categorical_columns += list(bin_trip_type_columns)\n",
    "binarized_categorical_columns += list(['PRODUCT'])\n",
    "\n",
    "print(data_binarized[binarized_categorical_columns].head())\n",
    "\n",
    "for i in range(0, len(binarized_categorical_columns)):\n",
    "    print(binarized_categorical_columns[i] + \" type:\", data_binarized[binarized_categorical_columns[i]].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the new cleaned and binarized data into a new CSV file called 'clean_binarized_train.csv'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_binarized.to_csv('clean_binarized_train.csv', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll try out a quick test with each clean dataset (encoded and binarized) and a simple classification model such as 'Logistic Regression' to see how the model performs. We'll split the data into training and test data, we'll train the model using the training data and the model's default parameters. For now we won't perform any grid-search to perform hyper-parameter tunning or any nested cross-validation to obtain a generalization score.\n",
    "\n",
    "We'll use the test data to evaluate the previous model with the F1 score and we'll plot the confussion matrix to visually understand better the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function that evaluates model using F1 score\n",
    "def evaluate_model(best_estimator, X_test, y_test):\n",
    "    y_pred = best_estimator.predict(X_test)    \n",
    "    \n",
    "    f1_eval_score = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    print(\"##### F1 Score:\", f1_eval_score)\n",
    "    print()\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plot_confusion_matrix(cm, ['False', 'True'], title='Confusion matrix', cmap=plt.cm.Purples)\n",
    "    \n",
    "\n",
    "# Function to plot confussion matrix\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Purples):\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    \n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        \n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaned Encoded Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load encoded data\n",
    "data_encoded = pd.read_csv('clean_encoded_train.csv', sep=';', decimal='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split data to separate 'y' and 'X'.\n",
    "de_y = data_encoded['EXTRA_BAGGAGE']\n",
    "\n",
    "# Drop target variable from X DataFrame\n",
    "de_X = data_encoded.drop('EXTRA_BAGGAGE', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into training and test datasets, asumming the 'test.csv' data is future data and the 'train.csv' data is the only one we have available.\n",
    "\n",
    "We'll use the 'stratify' option since previously we saw the target variable was imbalanced and we want to keep the distribution of each class in both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "de_X_train, de_X_test, de_y_train, de_y_test = train_test_split(de_X, de_y, test_size=0.3, random_state=875146, stratify=de_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "de_baseline = LogisticRegression(random_state=621473)\n",
    "\n",
    "de_baseline.fit(de_X_train, de_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evaluate_model(de_baseline, de_X_test, de_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we said we were going to use only the default parameters of the LogisticRegression model, we must consider using the 'class_weight' parameter. If we don't specify an option, the model assumes all classes have the same weight equal to one. Since we know a priori, our samples are imbalanced with respect to the response variable, the 'balanced' option automatically adjusts the weights of each sample inversely proportional to the class frequencies in the input data. Taking this into consideration, we train the model again with this option and see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "de_baseline = LogisticRegression(class_weight='balanced', random_state=621473)\n",
    "\n",
    "de_baseline.fit(de_X_train, de_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evaluate_model(de_baseline, de_X_test, de_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OJO! Decir algo sobre la diferencia entre scores y CM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaned Binarized Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load binarized data\n",
    "data_binarized = pd.read_csv('clean_binarized_train.csv', sep=';', decimal='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split data to separate 'y' and 'X'.\n",
    "db_y = data_binarized['EXTRA_BAGGAGE']\n",
    "\n",
    "# Drop target variable from X DataFrame\n",
    "db_X = data_binarized.drop('EXTRA_BAGGAGE', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db_X_train, db_X_test, db_y_train, db_y_test = train_test_split(db_X, db_y, test_size=0.3, random_state=875146, stratify=db_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "db_baseline = LogisticRegression(class_weight='balanced', random_state=621473)\n",
    "\n",
    "db_baseline.fit(db_X_train, db_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evaluate_model(db_baseline, db_X_test, db_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see that we obtain almost the same F1 score for both datasets. The encoded has a slightly better performance score so for further analysis we'll keep using this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering is the process of creating new variables from the existing raw features in order to increase the predictive power of the learning algorithms. In fact, during the cleaning process, having transformed the categorical variables to discrete or binary variables is already considered as feature engineering, this process is specifically known as 'factorization' of a categorical feature, but we did it earlier to test our data and create a first baseline model to comapre with. Another common feature engineering technique is called 'binning', which consists of 'cutting' values of a continuous variable into 'bins' so it becomes a categorical variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise we'll create three new variables using pre-existing variables from the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load original data\n",
    "original_data = pd.read_csv('train.csv', sep=';', decimal=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRIP_DAYS Feature\n",
    "\n",
    "This feature will be created using the 'DEPARTURE' and 'ARRIVAL' variables. It will represent the duration in days of the trip. We'll assume 'DEPARTURE' dates corresponde to the current year 2017. If the month of the 'ARRIVAL' date is smaller than the one in the 'DEPARTURE' date, we'll assume this date corresponds to next year 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_trip_days(row):\n",
    "    departure_date = datetime.strptime(row['DEPARTURE'] + \"/2017\", '%d/%B/%Y')\n",
    "    arrival_date = datetime.strptime(row['ARRIVAL'] + \"/2017\", '%d/%B/%Y')\n",
    "    \n",
    "    if arrival_date.month < departure_date.month:\n",
    "        arrival_date = datetime.strptime(row['ARRIVAL'] + \"/2018\", '%d/%B/%Y')\n",
    "    \n",
    "    trip_days = arrival_date - departure_date\n",
    "    \n",
    "    return trip_days.days\n",
    "\n",
    "trip_days = original_data.apply(lambda row: calculate_trip_days(row), axis=1)\n",
    "trip_days = trip_days[pd.notnull(original_data['DEVICE'])]\n",
    "trip_days = trip_days.reset_index(drop=True)\n",
    "\n",
    "data_encoded['TRIP_DAYS'] = trip_days\n",
    "\n",
    "print(data_encoded['TRIP_DAYS'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLANNING_DAYS Feature\n",
    "\n",
    "This feature will be created using the 'TIMESTAMP' and 'DEPARTURE' variables. It will represent how many days ahead of the departure date, the trip was planned, starting from the day the plane tickets were bought."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_planning_days(row):\n",
    "    buy_date = datetime.strptime(row['TIMESTAMP'] + \"/2017\", '%d/%B/%Y')\n",
    "    departure_date = datetime.strptime(row['DEPARTURE'] + \"/2017\", '%d/%B/%Y')\n",
    "    \n",
    "    if departure_date.month < buy_date.month:\n",
    "        departure_date = datetime.strptime(row['DEPARTURE'] + \"/2018\", '%d/%B/%Y')\n",
    "    \n",
    "    planning_days = departure_date - buy_date\n",
    "    \n",
    "    return planning_days.days\n",
    "\n",
    "planning_days = original_data.apply(lambda row: calculate_planning_days(row), axis=1)\n",
    "planning_days = planning_days[pd.notnull(original_data['DEVICE'])]\n",
    "planning_days = planning_days.reset_index(drop=True)\n",
    "\n",
    "data_encoded['PLANNING_DAYS'] = planning_days\n",
    "\n",
    "print(data_encoded['PLANNING_DAYS'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WEEKEND_DAYS Feature\n",
    "\n",
    "This feature will be created using the 'DEPARTURE' and 'ARRIVAL' variables. It will represent how many weekend days are included in the duration of the trip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_weekend_days(row):\n",
    "    aux = datetime.strptime(row['DEPARTURE'] + \"/2017\", '%d/%B/%Y')\n",
    "    arrival_date = datetime.strptime(row['ARRIVAL'] + \"/2017\", '%d/%B/%Y')\n",
    "    \n",
    "    if arrival_date.month < aux.month:\n",
    "        arrival_date = datetime.strptime(row['ARRIVAL'] + \"/2018\", '%d/%B/%Y')\n",
    "    \n",
    "    weekend_days = 0\n",
    "    \n",
    "    while (aux <= arrival_date):\n",
    "        if (aux.weekday() > 4):\n",
    "            weekend_days += 1\n",
    "            \n",
    "        aux = aux + timedelta(days=1)\n",
    "    \n",
    "    return weekend_days\n",
    "\n",
    "weekend_days = original_data.apply(lambda row: calculate_weekend_days(row), axis=1)\n",
    "weekend_days = weekend_days[pd.notnull(original_data['DEVICE'])]\n",
    "weekend_days = weekend_days.reset_index(drop=True)\n",
    "\n",
    "data_encoded['WEEKEND_DAYS'] = weekend_days\n",
    "\n",
    "print(data_encoded['WEEKEND_DAYS'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We think this kind of variables can give more explicit information to the model in order to perform a better discrimination between passengers that buy or not an extra baggage on their trip. We'll save this new dataset into a separate CSV file called 'clean_enriched_train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_encoded.to_csv('cleaned_enriched_train.csv', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll try this new data with our baseline LigisticRegression model to see if our new features improve the model's performance score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load enriched data\n",
    "data_enriched = pd.read_csv('cleaned_enriched_train.csv', sep=';', decimal='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split data to separate 'y' and 'X'.\n",
    "den_y = data_enriched['EXTRA_BAGGAGE']\n",
    "\n",
    "# Drop target variable from X DataFrame\n",
    "den_X = data_enriched.drop('EXTRA_BAGGAGE', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "den_X_train, den_X_test, den_y_train, den_y_test = train_test_split(den_X, den_y, test_size=0.3, random_state=875146, stratify=den_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "den_baseline = LogisticRegression(class_weight='balanced', random_state=621473)\n",
    "\n",
    "den_baseline.fit(den_X_train, den_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evaluate_model(den_baseline, den_X_test, den_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The enriched data improves the model's performance score by a very small fraction, so from now on we will use these new enriched data for further experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection and Evaluation\n",
    "\n",
    "In this section we'll test various classification algorithms with the same data (enriched) to compare their performance scores. In order to get the most accurate generalization score (most close to reality) of a model, with the least bias, at the same time we choose the best hyper-parameters, we must perform nested cross-validation. The outer corss-validation will be used to assess the performance of the model. For each of this outer folds we'll perform the inner cross-validations that will be used to determine the hyper-parameters in each fold.\n",
    "\n",
    "More of nested cross-validation can be seen in this paper: Gavin C. Cawley, Nicola L. C. Talbot, \"On Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation\", JMLR 11(Jul):2079−2107, 2010 http://jmlr.csail.mit.edu/papers/v11/cawley10a.html\n",
    "\n",
    "Once we identify the model with the highest generalization score, we re-execute only the inner cross-validation process that performs grid-search along with cross-validation to find the best configuration of hyper-parameters for that model and obtained a unique model trained with the whole trining dataset. In practice the cross-validation score obtained for this model will be slightly differ from the one obtained in the previous nested cross-validation process. We do the latter in order to be aware of the 'real' generalization score obtained without bias.\n",
    "\n",
    "This model will be the one we'll test with new unseed future data, in this case, the given 'test.csv' data.\n",
    "\n",
    "We'll make use of an important sklearn's class called 'Pipeline', tha sequentially applies a list of transformers to the data and finally applies and estimator model. The purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters. For all models we'll use a VarianceThreshold transformer followed by a StandardScaler transformer before the classification estimator.\n",
    "\n",
    "The VarianceThreshold bassically removed useless features that have zero variance.\n",
    "\n",
    "The StandardScaler, as it's name states, standardizes each column to have zero mean and unit variance. This step is important since we are dealing with features with different measurement units, and we don't want the estimator to give more importance to features with higher values because they're not standardized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "We first start by trying out again the LogistiRegression classifier but performing hyper-parameter tunning for the penalization parameter 'C'. This parameter represents the inverse of regularization strength. Similar to SVM's the smaller the values, the stronger the regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load cleaned enriched data\n",
    "data_type = 'clean_enriched'\n",
    "data = pd.read_csv(data_type + '_train.csv', sep=';', decimal='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split data to separate 'y' and 'X'.\n",
    "y = data['EXTRA_BAGGAGE']\n",
    "\n",
    "# Drop target variable from X DataFrame\n",
    "X = data.drop('EXTRA_BAGGAGE', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_pipe = Pipeline([('variance', VarianceThreshold()),\n",
    "                    ('scaler', StandardScaler()),\n",
    "                    ('lr', LogisticRegression(class_weight='balanced', random_state=621473))])\n",
    "\n",
    "param_grid = dict()\n",
    "param_grid['lr__C'] = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "\n",
    "inner_cv = StratifiedKFold(n_splits=10, random_state=975428)\n",
    "outer_cv = StratifiedKFold(n_splits=10, random_state=248733)\n",
    "\n",
    "gs_cv = GridSearchCV(lr_pipe, param_grid=param_grid, n_jobs=-1, scoring='f1_weighted', cv=inner_cv, verbose=10)\n",
    "\n",
    "lr_nested_cv_f1_scores = cross_val_score(gs_cv, X, y, cv=outer_cv, verbose=10)\n",
    "\n",
    "joblib.dump(lr_nested_cv_f1_scores, data_type + '_lr_nested_cv_f1_scores.pkl', compress=1)\n",
    "\n",
    "print(\"##### Generalization F1 Score: mean =\", np.mean(lr_nested_cv_f1_scores), \"std =\", np.std(lr_nested_cv_f1_scores))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous execution gives the following F1 generalization score:\n",
    "\n",
    "##### Generalization F1 Score: mean = 0.668949634197 std = 0.00667454322097\n",
    "\n",
    "We'll also plot the outer cross-validation scores in a boxplot to assess the model's statibility. If there are very few or no outlierts, this is a good sign that the model is stable. Since we have a small standard deviation we expect to have no outliers in the boxplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to plot boxplot of nested cross-validation scores.\n",
    "def plot_cv_scores(cv_scores):\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    \n",
    "    ax = sns.boxplot(x=cv_scores, orient=\"v\")\n",
    "    \n",
    "    plt.ylim(0, 1) \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr_nested_cv_f1_scores = joblib.load(data_type + '_lr_nested_cv_f1_scores.pkl')\n",
    "\n",
    "plot_cv_scores(lr_nested_cv_f1_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear SVM\n",
    "\n",
    "As a second linear classification model we'll try LinearSVC classifier, performing hyper-parameter tunning for the penalization parameter 'C'. This parameter represents penalty given to the error term. The smaller the values, the stronger the regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linear_svm_pipe = Pipeline([('variance', VarianceThreshold()),\n",
    "                            ('scaler', StandardScaler()),\n",
    "                            ('linear_svm', LinearSVC(penalty='l1', dual=False, random_state=123456, class_weight='balanced'))])\n",
    "\n",
    "param_grid = dict()\n",
    "param_grid['linear_svm__C'] = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "\n",
    "inner_cv = StratifiedKFold(n_splits=10, random_state=975428)\n",
    "outer_cv = StratifiedKFold(n_splits=10, random_state=248733)\n",
    "\n",
    "gs_cv = GridSearchCV(linear_svm_pipe, param_grid=param_grid, n_jobs=-1, scoring='f1_weighted', cv=inner_cv, verbose=10)\n",
    "\n",
    "linear_svm_nested_cv_f1_scores = cross_val_score(gs_cv, X, y, cv=outer_cv, verbose=10)\n",
    "\n",
    "joblib.dump(linear_svm_nested_cv_f1_scores, data_type + '_linear_svm_nested_cv_f1_scores.pkl', compress=1)\n",
    "\n",
    "print(\"##### Generalization F1 Score: mean =\", np.mean(linear_svm_nested_cv_f1_scores), \"std =\", np.std(linear_svm_nested_cv_f1_scores))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F1 generalization score is shown below:\n",
    "\n",
    "##### Generalization F1 Score: mean = 0.667330992787 std = 0.00671589397452"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "linear_svm_nested_cv_f1_scores = joblib.load(data_type + '_linear_svm_nested_cv_f1_scores.pkl')\n",
    "\n",
    "plot_cv_scores(linear_svm_nested_cv_f1_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting boxplot of the outer cross-validation scores still shows model stability even though the feneralization score obtained was a bit lower than the one obtained with LogisticRegression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "Finally we test a non-linear ensemble method called the RandomForestClassifier. We change the default value for the 'max_features' paranmeter to use a common rule of thumb, the 'sqrt' option. This parameter represents the number of features to consider when looking for the best split in a tree.\n",
    "\n",
    "We perform hyper-parameter tunning of several important parameters such as:\n",
    "\n",
    "- n_estimators: this parameter represents the number of trees we want to build before taking the maximum voting or averages of predictions. The higher the number of trees the more stronger and stable the predictions will be.\n",
    "- max_depth: this parameter represents the maximum depth of each tree.\n",
    "- min_samples_leaf: this parameter represents the minimum number of samples to consider in a leaf of a tree. A smaller leaf makes the model more prone to capturing noise in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_pipe = Pipeline([('variance', VarianceThreshold()),\n",
    "                            ('scaler', StandardScaler()),\n",
    "                            ('rf', RandomForestClassifier(max_features='sqrt', oob_score=True, random_state=573146, class_weight='balanced'))])\n",
    "\n",
    "param_grid = dict()\n",
    "param_grid['rf__n_estimators'] = list(range(200, 1200, 150))\n",
    "param_grid['rf__max_depth'] = [10, 20, 30]\n",
    "param_grid['rf__min_samples_leaf'] = [50, 60, 70, 80, 90, 100]\n",
    "\n",
    "\n",
    "inner_cv = StratifiedKFold(n_splits=10, random_state=975428)\n",
    "outer_cv = StratifiedKFold(n_splits=10, random_state=248733)\n",
    "\n",
    "gs_cv = GridSearchCV(rf_pipe, param_grid=param_grid, n_jobs=-1, scoring='f1_weighted', cv=inner_cv, verbose=10)\n",
    "\n",
    "rf_nested_cv_f1_scores = cross_val_score(gs_cv, X, y, cv=outer_cv, verbose=10)\n",
    "\n",
    "joblib.dump(rf_nested_cv_f1_scores, 'rf_nested_cv_f1_scores.pkl', compress=1)\n",
    "\n",
    "print(\"##### Generalization F1 Score: mean =\", np.mean(rf_nested_cv_f1_scores), \"std =\", np.std(rf_nested_cv_f1_scores))\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
